# Master project publication in Artificial Intelligence Review!

During my Master's in Computer Science, a mandatory course was the 'Literature Study'. Not the most exciting thing, but I was able to connect it to my Master Project (roughly said, at that point, Generating Social Media with LLMs) that I was going to perform under the supervision of dr. Petter Törnberg. I decided to read lots of papers on generative social simulations and inventarised what the simulations measured, how they performed validation and how aware they were on known validation challenges. After the course, Petter and I rewrote the assignment into a journal article which got [published in the critically acclaimed Artificial Intelligence Review](https://link.springer.com/article/10.1007/s10462-025-11412-6) this week (it was already on [arXiv](https://arxiv.org/abs/2504.03274)). For me, this was the first academic cycle of submitting, reviewers ([reviewer #2 kept true to the myth](https://www.reddit.com/r/AskAcademia/comments/gqzpax/how_did_the_myth_of_reviewer_2_come_to_be/)) and resubmitting. A huge thanks to Petter for doing a lot of the necessary work!

[Read the full publication here!](https://link.springer.com/article/10.1007/s10462-025-11412-6)

Out of laziness, here is the abstract of the paper (which is actually a really good audience-friendly summary):

Recent advances in Large Language Models (LLMs) have revitalized interest in Agent-Based Models (ABMs) by enabling “generative” simulations, with agents that can plan, reason, and interact through natural language. These developments promise greater realism and expressive power, but also revive long-standing concerns over empirical grounding, calibration, and validation—issues that have historically limited the uptake of ABMs in the social sciences. This paper systematically reviews the emerging literature on generative ABMs to assess how these long-standing challenges are being addressed. We map domains of application, categorize reported validation practices, and assess their alignment with the stated modeling goals. Our review suggests that the use of LLMs may exacerbate rather than alleviate the challenge of validating ABMs, given their black-box structure, cultural biases, and stochastic outputs. While the need for validation is increasingly acknowledged, studies often rely on face-validity or outcome measures that are only loosely tied to underlying mechanisms. Generative ABMs thus occupy an ambiguous methodological space—lacking both the parsimony of formal models and the empirical validity of data-driven approaches—and their contribution to cumulative social-scientific knowledge hinges on resolving this tension.

A second paper, based on the actual Master project, is still under review for a journal. However, this is also already on [arXiv](https://arxiv.org/abs/2508.03385) and is about testing prosocial interventions on social media using generative social simulation. This seems in contradiction with the paper just published, if you want to know how we solve this -> read the paper! The paper actually got a lot of media attention, for example in [Ars Technica](https://arstechnica.com/science/2025/08/study-social-media-probably-cant-be-fixed/), [Science Magazine](https://www.science.org/content/article/don-t-blame-algorithm-polarization-may-be-inherent-social-media), the Dutch [Bright](https://www.bright.nl/nieuws/1682371/nederlandse-studie-ook-zonder-algoritmes-zorgen-sociale-media-voor-polarisatie.html), [Futurism](https://futurism.com/social-network-ai-intervention-echo-chamber) and even in [Germany](https://t3n.de/news/echokammern-entstehen-von-selbst-1703271/).

![media attention](../images/futurism.png)